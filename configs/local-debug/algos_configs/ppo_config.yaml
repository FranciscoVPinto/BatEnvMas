#training
# lr: [0.00001,0.001] #uniform sample
num_sgd_iter: 30
train_batch_size: 128 #choice
# sgd_minibatch_size: [128,256] #integer sample

# entropy_coeff: [0.01,0.3] #uniform sample
lambda_: 1.0

kl_coeff: 0.2
kl_target: 0.01

vf_loss_coeff: 1.0

# vf_clip_param: [10.0,20.0] #choice
clip_param: 0.3


_enable_learner_api: False
model:
  #custom_model: cc_shift_mask
  custom_model: shift_mask
  #custom_model: lstm_model
  fcnet_hiddens: [256,256,256]
  fcnet_activation: relu
  custom_model_config:
    fcnet_hiddens: [128,128]


#environment
env: flexenv
disable_env_checking: True

#debugging
# seed: [42, 123, 256, 789, 1024] #grid search
seed: [1, 2]
log_level: DEBUG

#rollouts
num_rollout_workers: 1

#Framework
framework: tf2
eager_tracing: True

#RLModule
_enable_rl_module_api: False
enable_rl_module_and_learner: False

#Resources
#num_gpus_per_worker: 1


# vf_clip_param: 20.0
num_cpus_per_worker: 2
num_cpus_per_trainer_worker: 1
num_trainer_workers: 1

#Evaluation
# evaluation_interval: 1
# evaluation_num_workers: 1
# evaluation_num_episodes: 10



# num_rollout_workers: 1
num_workers: 2
# num_envs_per_worker: 2
# env_config:
#   reward_shaping: true
#   observation_filter: "NoFilter"
#   horizon: 100
#   action_repeat: 4
# gamma: 0.99
# lambda: 0.95
# lr: 0.0003
# num_sgd_iter: 10
# sgd_minibatch_size: 256
# train_batch_size: 2048
# timesteps_per_iteration: 2048
# model:
#   framework: tf
#   use_gae: true
#   vf_share_layers: true
#   max_seq_len: 32
#   learning_rate: 0.0003
#   num_layers: 2
#   hidden_dim: 64
#   activation: relu
#   use_critic: true
#   use_hiddens_for_v: true
#   vf_loss_coeff: 0.5
#   entropy_coeff: 0.01
#   grad_clip: 0.5
#   eps: 0.2
#   dueling: false
#   double_q: false
#   target_network_update_freq: 1000
#   target_network_mix: 0.01
#   use_moving_averages: false
#   use_exploration_bonus: false
#   exploration_bonus_coeff: 0.1
#   exploration_bonus_temperature: 1.0
#   use_softmax_exploration: false
#   use_expert_iter: false
#   use_proximal_policy_optimization: true
#   use_policy_delay: true
#   compress_observations: false
#   observation_filter: "NoFilter"
#   use_observation_normalization: true
#   use_recurrent_policy: false
#   num_recurrent_layers: 1
#   recurrent_activation: tanh
#   max_seq_len: 32
#   use_trajectory_view_buffer: false
#   use_trajectory_filters: false
#   trajectory_filter_threshold: 0.0
#   use_trajectory_checkpointer: false
#   use_pytree_state: false
#   use_deque_replay_buffer: false
