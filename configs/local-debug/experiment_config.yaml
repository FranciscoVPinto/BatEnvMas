# experiment config
exp_name: local-debug

train: True
resume: False


pol_type: agent_pol #shared_pol shared policy or each agent will have its own policy
# pol_type: shared_pol



algorithm:
  # name: PPO
  name: CentralizedCritic
  config: 'ppo_config.yaml'
environment_cls: 'FlexEnv'

#tune config
mode: min
metric: episode_reward_mean
# metric: /info/learner/pol_ag1/learner_stats/policy_loss

# HPO
hpo_algo: 'asha' #'pbt'
asha_params:
  n_samples: 1
  time_attribute: 'training_iteration'
  max_time: 100
  grace: 20
  red_factor: 3


# Run config
verbose: 0

#trainable config
n_iters: 101
checkpoint_freq: 10

#resources
num_cpu_head: 1.0
num_gpu_head: 0.0

num_cpu_node: 1.0
num_gpu_node: 0.0

n_factor: 4

spill_dir: /home/omega/Documents/FCUL/Projects/FlexEnvMas/raylog/spill1
